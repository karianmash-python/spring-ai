[
  {
    "url": "https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html",
    "markdown": "SearchCTRL + k\n\n# ETL Pipeline\n\nThe Extract, Transform, and Load (ETL) framework serves as the backbone of data processing within the Retrieval Augmented Generation (RAG) use case.\n\nThe ETL pipeline orchestrates the flow from raw data sources to a structured vector store, ensuring data is in the optimal format for retrieval by the AI model.\n\nThe RAG use case is text to augment the capabilities of generative models by retrieving relevant information from a body of data to enhance the quality and relevance of the generated output.\n\n## API Overview\n\nThe ETL pipelines creates, transforms and stores `Document` instances.\n\n![Spring AI Message API](https://docs.spring.io/spring-ai/reference/_images/spring-ai-document1-api.jpg)\n\nThe `Document` class contains text, metadata and optionally additional media types like images, audio and video.\n\nThere are three main components of the ETL pipeline,\n\n- `DocumentReader` that implements `Supplier<List<Document>>`\n\n- `DocumentTransformer` that implements `Function<List<Document>, List<Document>>`\n\n- `DocumentWriter` that implements `Consumer<List<Document>>`\n\n\nThe `Document` class content is created from PDFs, text files and other document types with the help of `DocumentReader`.\n\nTo construct a simple ETL pipeline, you can chain together an instance of each type.\n\n![etl pipeline](https://docs.spring.io/spring-ai/reference/_images/etl-pipeline.jpg)\n\nLet\u2019s say we have the following instances of those three ETL types\n\n- `PagePdfDocumentReader` an implementation of `DocumentReader`\n\n- `TokenTextSplitter` an implementation of `DocumentTransformer`\n\n- `VectorStore` an implementation of `DocumentWriter`\n\n\nTo perform the basic loading of data into a Vector Database for use with the Retrieval Augmented Generation pattern, use the following code in Java function style syntax.\n\n```java hljs\nvectorStore.accept(tokenTextSplitter.apply(pdfReader.get()));\nCopied!\n```\n\nAlternatively, you can use method names that are more naturally expressive for the domain\n\n```java hljs\nvectorStore.write(tokenTextSplitter.split(pdfReader.read()));\nCopied!\n```\n\n## ETL Interfaces\n\nThe ETL pipeline is composed of the following interfaces and implementations.\nDetailed ETL class diagram is shown in the [ETL Class Diagram](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#etl-class-diagram) section.\n\n### DocumentReader\n\nProvides a source of documents from diverse origins.\n\n```java hljs\npublic interface DocumentReader extends Supplier<List<Document>> {\n\n    default List<Document> read() {\n\t\treturn get();\n\t}\n}\nCopied!\n```\n\n### DocumentTransformer\n\nTransforms a batch of documents as part of the processing workflow.\n\n```java hljs\npublic interface DocumentTransformer extends Function<List<Document>, List<Document>> {\n\n    default List<Document> transform(List<Document> transform) {\n\t\treturn apply(transform);\n\t}\n}\nCopied!\n```\n\n### DocumentWriter\n\nManages the final stage of the ETL process, preparing documents for storage.\n\n```java hljs\npublic interface DocumentWriter extends Consumer<List<Document>> {\n\n    default void write(List<Document> documents) {\n\t\taccept(documents);\n\t}\n}\nCopied!\n```\n\n### ETL Class Diagram\n\nThe following class diagram illustrates the ETL interfaces and implementations.\n\n![etl class diagram](https://docs.spring.io/spring-ai/reference/_images/etl-class-diagram.jpg)\n\n## DocumentReaders\n\n### JSON\n\nThe `JsonReader` processes JSON documents, converting them into a list of `Document` objects.\n\n#### Example\n\n```java hljs\n@Component\nclass MyJsonReader {\n\n\tprivate final Resource resource;\n\n    MyJsonReader(@Value(\"classpath:bikes.json\") Resource resource) {\n        this.resource = resource;\n    }\n\n\tList<Document> loadJsonAsDocuments() {\n        JsonReader jsonReader = new JsonReader(this.resource, \"description\", \"content\");\n        return jsonReader.get();\n\t}\n}\nCopied!\n```\n\n#### Constructor Options\n\nThe `JsonReader` provides several constructor options:\n\n1. `JsonReader(Resource resource)`\n\n2. `JsonReader(Resource resource, String\u2026\u200b jsonKeysToUse)`\n\n3. `JsonReader(Resource resource, JsonMetadataGenerator jsonMetadataGenerator, String\u2026\u200b jsonKeysToUse)`\n\n\n#### Parameters\n\n- `resource`: A Spring `Resource` object pointing to the JSON file.\n\n- `jsonKeysToUse`: An array of keys from the JSON that should be used as the text content in the resulting `Document` objects.\n\n- `jsonMetadataGenerator`: An optional `JsonMetadataGenerator` to create metadata for each `Document`.\n\n\n#### Behavior\n\nThe `JsonReader` processes JSON content as follows:\n\n- It can handle both JSON arrays and single JSON objects.\n\n- For each JSON object (either in an array or a single object):\n\n\n\n- It extracts the content based on the specified `jsonKeysToUse`.\n\n- If no keys are specified, it uses the entire JSON object as content.\n\n- It generates metadata using the provided `JsonMetadataGenerator` (or an empty one if not provided).\n\n- It creates a `Document` object with the extracted content and metadata.\n\n\n#### Using JSON Pointers\n\nThe `JsonReader` now supports retrieving specific parts of a JSON document using JSON Pointers. This feature allows you to easily extract nested data from complex JSON structures.\n\n##### The `get(String pointer)` method\n\n```java hljs\npublic List<Document> get(String pointer)\nCopied!\n```\n\nThis method allows you to use a JSON Pointer to retrieve a specific part of the JSON document.\n\n###### Parameters\n\n- `pointer`: A JSON Pointer string (as defined in RFC 6901) to locate the desired element within the JSON structure.\n\n\n###### Return Value\n\n- Returns a `List<Document>` containing the documents parsed from the JSON element located by the pointer.\n\n\n###### Behavior\n\n- The method uses the provided JSON Pointer to navigate to a specific location in the JSON structure.\n\n- If the pointer is valid and points to an existing element:\n\n\n\n- For a JSON object: it returns a list with a single Document.\n\n- For a JSON array: it returns a list of Documents, one for each element in the array.\n\n\n- If the pointer is invalid or points to a non-existent element, it throws an `IllegalArgumentException`.\n\n\n###### Example\n\n```java hljs\nJsonReader jsonReader = new JsonReader(resource, \"description\");\nList<Document> documents = this.jsonReader.get(\"/store/books/0\");\nCopied!\n```\n\n#### Example JSON Structure\n\n```json hljs\n[\\\n  {\\\n    \"id\": 1,\\\n    \"brand\": \"Trek\",\\\n    \"description\": \"A high-performance mountain bike for trail riding.\"\\\n  },\\\n  {\\\n    \"id\": 2,\\\n    \"brand\": \"Cannondale\",\\\n    \"description\": \"An aerodynamic road bike for racing enthusiasts.\"\\\n  }\\\n]\nCopied!\n```\n\nIn this example, if the `JsonReader` is configured with `\"description\"` as the `jsonKeysToUse`, it will create `Document` objects where the content is the value of the \"description\" field for each bike in the array.\n\n#### Notes\n\n- The `JsonReader` uses Jackson for JSON parsing.\n\n- It can handle large JSON files efficiently by using streaming for arrays.\n\n- If multiple keys are specified in `jsonKeysToUse`, the content will be a concatenation of the values for those keys.\n\n- The reader is flexible and can be adapted to various JSON structures by customizing the `jsonKeysToUse` and `JsonMetadataGenerator`.\n\n\n### Text\n\nThe `TextReader` processes plain text documents, converting them into a list of `Document` objects.\n\n#### Example\n\n```java hljs\n@Component\nclass MyTextReader {\n\n    private final Resource resource;\n\n    MyTextReader(@Value(\"classpath:text-source.txt\") Resource resource) {\n        this.resource = resource;\n    }\n\n\tList<Document> loadText() {\n\t\tTextReader textReader = new TextReader(this.resource);\n\t\ttextReader.getCustomMetadata().put(\"filename\", \"text-source.txt\");\n\n\t\treturn textReader.read();\n    }\n}\nCopied!\n```\n\n#### Constructor Options\n\nThe `TextReader` provides two constructor options:\n\n1. `TextReader(String resourceUrl)`\n\n2. `TextReader(Resource resource)`\n\n\n#### Parameters\n\n- `resourceUrl`: A string representing the URL of the resource to be read.\n\n- `resource`: A Spring `Resource` object pointing to the text file.\n\n\n#### Configuration\n\n- `setCharset(Charset charset)`: Sets the character set used for reading the text file. Default is UTF-8.\n\n- `getCustomMetadata()`: Returns a mutable map where you can add custom metadata for the documents.\n\n\n#### Behavior\n\nThe `TextReader` processes text content as follows:\n\n- It reads the entire content of the text file into a single `Document` object.\n\n- The content of the file becomes the content of the `Document`.\n\n- Metadata is automatically added to the `Document`:\n\n\n\n- `charset`: The character set used to read the file (default: \"UTF-8\").\n\n- `source`: The filename of the source text file.\n\n\n- Any custom metadata added via `getCustomMetadata()` is included in the `Document`.\n\n\n#### Notes\n\n- The `TextReader` reads the entire file content into memory, so it may not be suitable for very large files.\n\n- If you need to split the text into smaller chunks, you can use a text splitter like `TokenTextSplitter` after reading the document:\n\n\n```java hljs\nList<Document> documents = textReader.get();\nList<Document> splitDocuments = new TokenTextSplitter().apply(this.documents);\nCopied!\n```\n\n- The reader uses Spring\u2019s `Resource` abstraction, allowing it to read from various sources (classpath, file system, URL, etc.).\n\n- Custom metadata can be added to all documents created by the reader using the `getCustomMetadata()` method.\n\n\n### HTML (JSoup)\n\nThe `JsoupDocumentReader` processes HTML documents, converting them into a list of `Document` objects using the JSoup library.\n\n#### Example\n\n```java hljs\n@Component\nclass MyHtmlReader {\n\n    private final Resource resource;\n\n    MyHtmlReader(@Value(\"classpath:/my-page.html\") Resource resource) {\n        this.resource = resource;\n    }\n\n    List<Document> loadHtml() {\n        JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder()\n            .selector(\"article p\") // Extract paragraphs within <article> tags\n            .charset(\"ISO-8859-1\")  // Use ISO-8859-1 encoding\n            .includeLinkUrls(true) // Include link URLs in metadata\n            .metadataTags(List.of(\"author\", \"date\")) // Extract author and date meta tags\n            .additionalMetadata(\"source\", \"my-page.html\") // Add custom metadata\n            .build();\n\n        JsoupDocumentReader reader = new JsoupDocumentReader(this.resource, config);\n        return reader.get();\n    }\n}\nCopied!\n```\n\nThe `JsoupDocumentReaderConfig` allows you to customize the behavior of the `JsoupDocumentReader`:\n\n- `charset`: Specifies the character encoding of the HTML document (defaults to \"UTF-8\").\n\n- `selector`: A JSoup CSS selector to specify which elements to extract text from (defaults to \"body\").\n\n- `separator`: The string used to join text from multiple selected elements (defaults to \"\\\\n\").\n\n- `allElements`: If `true`, extracts all text from the `<body>` element, ignoring the `selector` (defaults to `false`).\n\n- `groupByElement`: If `true`, creates a separate `Document` for each element matched by the `selector` (defaults to `false`).\n\n- `includeLinkUrls`: If `true`, extracts absolute link URLs and adds them to the metadata (defaults to `false`).\n\n- `metadataTags`: A list of `<meta>` tag names to extract content from (defaults to `[\"description\", \"keywords\"]`).\n\n- `additionalMetadata`: Allows you to add custom metadata to all created `Document` objects.\n\n\n#### Sample Document: my-page.html\n\n```html hljs xml\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>My Web Page</title>\n    <meta name=\"description\" content=\"A sample web page for Spring AI\">\n    <meta name=\"keywords\" content=\"spring, ai, html, example\">\n    <meta name=\"author\" content=\"John Doe\">\n    <meta name=\"date\" content=\"2024-01-15\">\n    <link rel=\"stylesheet\" href=\"style.css\">\n</head>\n<body>\n    <header>\n        <h1>Welcome to My Page</h1>\n    </header>\n    <nav>\n        <ul>\n            <li><a href=\"/\">Home</a></li>\n            <li><a href=\"/about\">About</a></li>\n        </ul>\n    </nav>\n    <article>\n        <h2>Main Content</h2>\n        <p>This is the main content of my web page.</p>\n        <p>It contains multiple paragraphs.</p>\n        <a href=\"https://www.example.com\">External Link</a>\n    </article>\n    <footer>\n        <p>&copy; 2024 John Doe</p>\n    </footer>\n</body>\n</html>\nCopied!\n```\n\nBehavior:\n\nThe `JsoupDocumentReader` processes the HTML content and creates `Document` objects based on the configuration:\n\n- The `selector` determines which elements are used for text extraction.\n\n- If `allElements` is `true`, all text within the `<body>` is extracted into a single `Document`.\n\n- If `groupByElement` is `true`, each element matching the `selector` creates a separate `Document`.\n\n- If neither `allElements` nor `groupByElement` is `true`, text from all elements matching the `selector` is joined using the `separator`.\n\n- The document title, content from specified `<meta>` tags, and (optionally) link URLs are added to the `Document` metadata.\n\n- The base URI, for resolving relative links, will be extracted from URL resources.\n\n\nThe reader preserves the text content of the selected elements, but removes any HTML tags within them.\n\n### Markdown\n\nThe `MarkdownDocumentReader` processes Markdown documents, converting them into a list of `Document` objects.\n\n#### Example\n\n```java hljs\n@Component\nclass MyMarkdownReader {\n\n    private final Resource resource;\n\n    MyMarkdownReader(@Value(\"classpath:code.md\") Resource resource) {\n        this.resource = resource;\n    }\n\n    List<Document> loadMarkdown() {\n        MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder()\n            .withHorizontalRuleCreateDocument(true)\n            .withIncludeCodeBlock(false)\n            .withIncludeBlockquote(false)\n            .withAdditionalMetadata(\"filename\", \"code.md\")\n            .build();\n\n        MarkdownDocumentReader reader = new MarkdownDocumentReader(this.resource, config);\n        return reader.get();\n    }\n}\nCopied!\n```\n\nThe `MarkdownDocumentReaderConfig` allows you to customize the behavior of the MarkdownDocumentReader:\n\n- `horizontalRuleCreateDocument`: When set to `true`, horizontal rules in the Markdown will create new `Document` objects.\n\n- `includeCodeBlock`: When set to `true`, code blocks will be included in the same `Document` as the surrounding text. When `false`, code blocks create separate `Document` objects.\n\n- `includeBlockquote`: When set to `true`, blockquotes will be included in the same `Document` as the surrounding text. When `false`, blockquotes create separate `Document` objects.\n\n- `additionalMetadata`: Allows you to add custom metadata to all created `Document` objects.\n\n\n#### Sample Document: code.md\n\n````markdown hljs\nThis is a Java sample application:\n\n```java\npackage com.example.demo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DemoApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(DemoApplication.class, args);\n    }\n}\n```\n\nMarkdown also provides the possibility to `use inline code formatting throughout` the entire sentence.\n\n---\n\nAnother possibility is to set block code without specific highlighting:\n\n```\n./mvnw spring-javaformat:apply\n```\nCopied!\n````\n\nBehavior: The MarkdownDocumentReader processes the Markdown content and creates Document objects based on the configuration:\n\n- Headers become metadata in the Document objects.\n\n- Paragraphs become the content of Document objects.\n\n- Code blocks can be separated into their own Document objects or included with surrounding text.\n\n- Blockquotes can be separated into their own Document objects or included with surrounding text.\n\n- Horizontal rules can be used to split the content into separate Document objects.\n\n\nThe reader preserves formatting like inline code, lists, and text styling within the content of the Document objects.\n\n### PDF Page\n\nThe `PagePdfDocumentReader` uses Apache PdfBox library to parse PDF documents\n\nAdd the dependency to your project using Maven or Gradle.\n\n```xml hljs\n<dependency>\n    <groupId>org.springframework.ai</groupId>\n    <artifactId>spring-ai-pdf-document-reader</artifactId>\n</dependency>\nCopied!\n```\n\nor to your Gradle `build.gradle` build file.\n\n```groovy hljs\ndependencies {\n    implementation 'org.springframework.ai:spring-ai-pdf-document-reader'\n}\nCopied!\n```\n\n#### Example\n\n```java hljs\n@Component\npublic class MyPagePdfDocumentReader {\n\n\tList<Document> getDocsFromPdf() {\n\n\t\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t\t.build())\n\t\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t\t.build());\n\n\t\treturn pdfReader.read();\n    }\n\n}\nCopied!\n```\n\n### PDF Paragraph\n\nThe `ParagraphPdfDocumentReader` uses the PDF catalog (e.g. TOC) information to split the input PDF into text paragraphs and output a single `Document` per paragraph.\nNOTE: Not all PDF documents contain the PDF catalog.\n\n#### Dependencies\n\nAdd the dependency to your project using Maven or Gradle.\n\n```xml hljs\n<dependency>\n    <groupId>org.springframework.ai</groupId>\n    <artifactId>spring-ai-pdf-document-reader</artifactId>\n</dependency>\nCopied!\n```\n\nor to your Gradle `build.gradle` build file.\n\n```groovy hljs\ndependencies {\n    implementation 'org.springframework.ai:spring-ai-pdf-document-reader'\n}\nCopied!\n```\n\n#### Example\n\n```java hljs\n@Component\npublic class MyPagePdfDocumentReader {\n\n\tList<Document> getDocsFromPdfWithCatalog() {\n\n        ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(\"classpath:/sample1.pdf\",\n                PdfDocumentReaderConfig.builder()\n                    .withPageTopMargin(0)\n                    .withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n                        .withNumberOfTopTextLinesToDelete(0)\n                        .build())\n                    .withPagesPerDocument(1)\n                    .build());\n\n\t    return pdfReader.read();\n    }\n}\nCopied!\n```\n\n### Tika (DOCX, PPTX, HTML\u2026\u200b)\n\nThe `TikaDocumentReader` uses Apache Tika to extract text from a variety of document formats, such as PDF, DOC/DOCX, PPT/PPTX, and HTML. For a comprehensive list of supported formats, refer to the [Tika documentation](https://tika.apache.org/3.1.0/formats.html).\n\n#### Dependencies\n\n```xml hljs\n<dependency>\n    <groupId>org.springframework.ai</groupId>\n    <artifactId>spring-ai-tika-document-reader</artifactId>\n</dependency>\nCopied!\n```\n\nor to your Gradle `build.gradle` build file.\n\n```groovy hljs\ndependencies {\n    implementation 'org.springframework.ai:spring-ai-tika-document-reader'\n}\nCopied!\n```\n\n#### Example\n\n```java hljs\n@Component\nclass MyTikaDocumentReader {\n\n    private final Resource resource;\n\n    MyTikaDocumentReader(@Value(\"classpath:/word-sample.docx\")\n                            Resource resource) {\n        this.resource = resource;\n    }\n\n    List<Document> loadText() {\n        TikaDocumentReader tikaDocumentReader = new TikaDocumentReader(this.resource);\n        return tikaDocumentReader.read();\n    }\n}\nCopied!\n```\n\n## Transformers\n\n### TextSplitter\n\nThe `TextSplitter` an abstract base class that helps divides documents to fit the AI model\u2019s context window.\n\n### TokenTextSplitter\n\nThe `TokenTextSplitter` is an implementation of `TextSplitter` that splits text into chunks based on token count, using the CL100K\\_BASE encoding.\n\n#### Usage\n\n```java hljs\n@Component\nclass MyTokenTextSplitter {\n\n    public List<Document> splitDocuments(List<Document> documents) {\n        TokenTextSplitter splitter = new TokenTextSplitter();\n        return splitter.apply(documents);\n    }\n\n    public List<Document> splitCustomized(List<Document> documents) {\n        TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true);\n        return splitter.apply(documents);\n    }\n}\nCopied!\n```\n\n#### Constructor Options\n\nThe `TokenTextSplitter` provides two constructor options:\n\n1. `TokenTextSplitter()`: Creates a splitter with default settings.\n\n2. `TokenTextSplitter(int defaultChunkSize, int minChunkSizeChars, int minChunkLengthToEmbed, int maxNumChunks, boolean keepSeparator)`\n\n\n#### Parameters\n\n- `defaultChunkSize`: The target size of each text chunk in tokens (default: 800).\n\n- `minChunkSizeChars`: The minimum size of each text chunk in characters (default: 350).\n\n- `minChunkLengthToEmbed`: The minimum length of a chunk to be included (default: 5).\n\n- `maxNumChunks`: The maximum number of chunks to generate from a text (default: 10000).\n\n- `keepSeparator`: Whether to keep separators (like newlines) in the chunks (default: true).\n\n\n#### Behavior\n\nThe `TokenTextSplitter` processes text content as follows:\n\n1. It encodes the input text into tokens using the CL100K\\_BASE encoding.\n\n2. It splits the encoded text into chunks based on the `defaultChunkSize`.\n\n3. For each chunk:\n\n\n\n1. It decodes the chunk back into text.\n\n2. It attempts to find a suitable break point (period, question mark, exclamation mark, or newline) after the `minChunkSizeChars`.\n\n3. If a break point is found, it truncates the chunk at that point.\n\n4. It trims the chunk and optionally removes newline characters based on the `keepSeparator` setting.\n\n5. If the resulting chunk is longer than `minChunkLengthToEmbed`, it\u2019s added to the output.\n\n\n4. This process continues until all tokens are processed or `maxNumChunks` is reached.\n\n5. Any remaining text is added as a final chunk if it\u2019s longer than `minChunkLengthToEmbed`.\n\n\n#### Example\n\n```java hljs\nDocument doc1 = new Document(\"This is a long piece of text that needs to be split into smaller chunks for processing.\",\n        Map.of(\"source\", \"example.txt\"));\nDocument doc2 = new Document(\"Another document with content that will be split based on token count.\",\n        Map.of(\"source\", \"example2.txt\"));\n\nTokenTextSplitter splitter = new TokenTextSplitter();\nList<Document> splitDocuments = this.splitter.apply(List.of(this.doc1, this.doc2));\n\nfor (Document doc : splitDocuments) {\n    System.out.println(\"Chunk: \" + doc.getContent());\n    System.out.println(\"Metadata: \" + doc.getMetadata());\n}\nCopied!\n```\n\n#### Notes\n\n- The `TokenTextSplitter` uses the CL100K\\_BASE encoding from the `jtokkit` library, which is compatible with newer OpenAI models.\n\n- The splitter attempts to create semantically meaningful chunks by breaking at sentence boundaries where possible.\n\n- Metadata from the original documents is preserved and copied to all chunks derived from that document.\n\n- The content formatter (if set) from the original document is also copied to the derived chunks if `copyContentFormatter` is set to `true` (default behavior).\n\n- This splitter is particularly useful for preparing text for large language models that have token limits, ensuring that each chunk is within the model\u2019s processing capacity.\n\n\n### ContentFormatTransformer\n\nEnsures uniform content formats across all documents.\n\n### KeywordMetadataEnricher\n\nThe `KeywordMetadataEnricher` is a `DocumentTransformer` that uses a generative AI model to extract keywords from document content and add them as metadata.\n\n#### Usage\n\n```java hljs\n@Component\nclass MyKeywordEnricher {\n\n    private final ChatModel chatModel;\n\n    MyKeywordEnricher(ChatModel chatModel) {\n        this.chatModel = chatModel;\n    }\n\n    List<Document> enrichDocuments(List<Document> documents) {\n        KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(this.chatModel, 5);\n        return enricher.apply(documents);\n    }\n}\nCopied!\n```\n\n#### Constructor\n\nThe `KeywordMetadataEnricher` constructor takes two parameters:\n\n1. `ChatModel chatModel`: The AI model used for generating keywords.\n\n2. `int keywordCount`: The number of keywords to extract for each document.\n\n\n#### Behavior\n\nThe `KeywordMetadataEnricher` processes documents as follows:\n\n1. For each input document, it creates a prompt using the document\u2019s content.\n\n2. It sends this prompt to the provided `ChatModel` to generate keywords.\n\n3. The generated keywords are added to the document\u2019s metadata under the key \"excerpt\\_keywords\".\n\n4. The enriched documents are returned.\n\n\n#### Customization\n\nThe keyword extraction prompt can be customized by modifying the `KEYWORDS_TEMPLATE` constant in the class. The default template is:\n\n```java hljs\n\\{context_str}. Give %s unique keywords for this document. Format as comma separated. Keywords:\nCopied!\n```\n\nWhere `{context_str}` is replaced with the document content, and `%s` is replaced with the specified keyword count.\n\n#### Example\n\n```java hljs\nChatModel chatModel = // initialize your chat model\nKeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5);\n\nDocument doc = new Document(\"This is a document about artificial intelligence and its applications in modern technology.\");\n\nList<Document> enrichedDocs = enricher.apply(List.of(this.doc));\n\nDocument enrichedDoc = this.enrichedDocs.get(0);\nString keywords = (String) this.enrichedDoc.getMetadata().get(\"excerpt_keywords\");\nSystem.out.println(\"Extracted keywords: \" + keywords);\nCopied!\n```\n\n#### Notes\n\n- The `KeywordMetadataEnricher` requires a functioning `ChatModel` to generate keywords.\n\n- The keyword count must be 1 or greater.\n\n- The enricher adds the \"excerpt\\_keywords\" metadata field to each processed document.\n\n- The generated keywords are returned as a comma-separated string.\n\n- This enricher is particularly useful for improving document searchability and for generating tags or categories for documents.\n\n\n### SummaryMetadataEnricher\n\nThe `SummaryMetadataEnricher` is a `DocumentTransformer` that uses a generative AI model to create summaries for documents and add them as metadata. It can generate summaries for the current document, as well as adjacent documents (previous and next).\n\n#### Usage\n\n```java hljs\n@Configuration\nclass EnricherConfig {\n\n    @Bean\n    public SummaryMetadataEnricher summaryMetadata(OpenAiChatModel aiClient) {\n        return new SummaryMetadataEnricher(aiClient,\n            List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT));\n    }\n}\n\n@Component\nclass MySummaryEnricher {\n\n    private final SummaryMetadataEnricher enricher;\n\n    MySummaryEnricher(SummaryMetadataEnricher enricher) {\n        this.enricher = enricher;\n    }\n\n    List<Document> enrichDocuments(List<Document> documents) {\n        return this.enricher.apply(documents);\n    }\n}\nCopied!\n```\n\n#### Constructor\n\nThe `SummaryMetadataEnricher` provides two constructors:\n\n1. `SummaryMetadataEnricher(ChatModel chatModel, List<SummaryType> summaryTypes)`\n\n2. `SummaryMetadataEnricher(ChatModel chatModel, List<SummaryType> summaryTypes, String summaryTemplate, MetadataMode metadataMode)`\n\n\n#### Parameters\n\n- `chatModel`: The AI model used for generating summaries.\n\n- `summaryTypes`: A list of `SummaryType` enum values indicating which summaries to generate (PREVIOUS, CURRENT, NEXT).\n\n- `summaryTemplate`: A custom template for summary generation (optional).\n\n- `metadataMode`: Specifies how to handle document metadata when generating summaries (optional).\n\n\n#### Behavior\n\nThe `SummaryMetadataEnricher` processes documents as follows:\n\n1. For each input document, it creates a prompt using the document\u2019s content and the specified summary template.\n\n2. It sends this prompt to the provided `ChatModel` to generate a summary.\n\n3. Depending on the specified `summaryTypes`, it adds the following metadata to each document:\n\n\n\n- `section_summary`: Summary of the current document.\n\n- `prev_section_summary`: Summary of the previous document (if available and requested).\n\n- `next_section_summary`: Summary of the next document (if available and requested).\n\n\n4. The enriched documents are returned.\n\n\n#### Customization\n\nThe summary generation prompt can be customized by providing a custom `summaryTemplate`. The default template is:\n\n```java hljs\n\"\"\"\nHere is the content of the section:\n{context_str}\n\nSummarize the key topics and entities of the section.\n\nSummary:\n\"\"\"\nCopied!\n```\n\n#### Example\n\n```java hljs\nChatModel chatModel = // initialize your chat model\nSummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel,\n    List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT));\n\nDocument doc1 = new Document(\"Content of document 1\");\nDocument doc2 = new Document(\"Content of document 2\");\n\nList<Document> enrichedDocs = enricher.apply(List.of(this.doc1, this.doc2));\n\n// Check the metadata of the enriched documents\nfor (Document doc : enrichedDocs) {\n    System.out.println(\"Current summary: \" + doc.getMetadata().get(\"section_summary\"));\n    System.out.println(\"Previous summary: \" + doc.getMetadata().get(\"prev_section_summary\"));\n    System.out.println(\"Next summary: \" + doc.getMetadata().get(\"next_section_summary\"));\n}\nCopied!\n```\n\nThe provided example demonstrates the expected behavior:\n\n- For a list of two documents, both documents receive a `section_summary`.\n\n- The first document receives a `next_section_summary` but no `prev_section_summary`.\n\n- The second document receives a `prev_section_summary` but no `next_section_summary`.\n\n- The `section_summary` of the first document matches the `prev_section_summary` of the second document.\n\n- The `next_section_summary` of the first document matches the `section_summary` of the second document.\n\n\n#### Notes\n\n- The `SummaryMetadataEnricher` requires a functioning `ChatModel` to generate summaries.\n\n- The enricher can handle document lists of any size, properly handling edge cases for the first and last documents.\n\n- This enricher is particularly useful for creating context-aware summaries, allowing for better understanding of document relationships in a sequence.\n\n- The `MetadataMode` parameter allows control over how existing metadata is incorporated into the summary generation process.\n\n\n## Writers\n\n### File\n\nThe `FileDocumentWriter` is a `DocumentWriter` implementation that writes the content of a list of `Document` objects into a file.\n\n#### Usage\n\n```java hljs\n@Component\nclass MyDocumentWriter {\n\n    public void writeDocuments(List<Document> documents) {\n        FileDocumentWriter writer = new FileDocumentWriter(\"output.txt\", true, MetadataMode.ALL, false);\n        writer.accept(documents);\n    }\n}\nCopied!\n```\n\n#### Constructors\n\nThe `FileDocumentWriter` provides three constructors:\n\n1. `FileDocumentWriter(String fileName)`\n\n2. `FileDocumentWriter(String fileName, boolean withDocumentMarkers)`\n\n3. `FileDocumentWriter(String fileName, boolean withDocumentMarkers, MetadataMode metadataMode, boolean append)`\n\n\n#### Parameters\n\n- `fileName`: The name of the file to write the documents to.\n\n- `withDocumentMarkers`: Whether to include document markers in the output (default: false).\n\n- `metadataMode`: Specifies what document content to be written to the file (default: MetadataMode.NONE).\n\n- `append`: If true, data will be written to the end of the file rather than the beginning (default: false).\n\n\n#### Behavior\n\nThe `FileDocumentWriter` processes documents as follows:\n\n1. It opens a FileWriter for the specified file name.\n\n2. For each document in the input list:\n\n\n\n1. If `withDocumentMarkers` is true, it writes a document marker including the document index and page numbers.\n\n2. It writes the formatted content of the document based on the specified `metadataMode`.\n\n\n3. The file is closed after all documents have been written.\n\n\n#### Document Markers\n\nWhen `withDocumentMarkers` is set to true, the writer includes markers for each document in the following format:\n\n```none hljs\n### Doc: [index], pages:[start_page_number,end_page_number]\nCopied!\n```\n\n#### Metadata Handling\n\nThe writer uses two specific metadata keys:\n\n- `page_number`: Represents the starting page number of the document.\n\n- `end_page_number`: Represents the ending page number of the document.\n\n\nThese are used when writing document markers.\n\n#### Example\n\n```java hljs\nList<Document> documents = // initialize your documents\nFileDocumentWriter writer = new FileDocumentWriter(\"output.txt\", true, MetadataMode.ALL, true);\nwriter.accept(documents);\nCopied!\n```\n\nThis will write all documents to \"output.txt\", including document markers, using all available metadata, and appending to the file if it already exists.\n\n#### Notes\n\n- The writer uses `FileWriter`, so it writes text files with the default character encoding of the operating system.\n\n- If an error occurs during writing, a `RuntimeException` is thrown with the original exception as its cause.\n\n- The `metadataMode` parameter allows control over how existing metadata is incorporated into the written content.\n\n- This writer is particularly useful for debugging or creating human-readable outputs of document collections.\n\n\n### VectorStore\n\nProvides integration with various vector stores.\nSee [Vector DB Documentation](https://docs.spring.io/spring-ai/reference/api/vectordbs.html) for a full listing."
  },
  {
    "url": "https://docs.spring.io/spring-ai/reference/api/retrieval-augmented-generation.html",
    "markdown": "SearchCTRL + k\n\n# Retrieval Augmented Generation\n\nRetrieval Augmented Generation (RAG) is a technique useful to overcome the limitations of large language models\nthat struggle with long-form content, factual accuracy, and context-awareness.\n\nSpring AI supports RAG by providing a modular architecture that allows you to build custom RAG flows yourself\nor use out-of-the-box RAG flows using the `Advisor` API.\n\n|     |     |\n| --- | --- |\n|  | Learn more about Retrieval Augmented Generation in the [concepts](https://docs.spring.io/spring-ai/reference/concepts.html#concept-rag) section. |\n\n## Advisors\n\nSpring AI provides out-of-the-box support for common RAG flows using the `Advisor` API.\n\nTo use the `QuestionAnswerAdvisor` or `RetrievalAugmentationAdvisor`, you need to add the `spring-ai-advisors-vector-store` dependency to your project:\n\n```xml hljs\n<dependency>\n   <groupId>org.springframework.ai</groupId>\n   <artifactId>spring-ai-advisors-vector-store</artifactId>\n</dependency>\nCopied!\n```\n\n### QuestionAnswerAdvisor\n\nA vector database stores data that the AI model is unaware of. When a user question is sent to the AI model, a `QuestionAnswerAdvisor` queries the vector database for documents related to the user question.\n\nThe response from the vector database is appended to the user text to provide context for the AI model to generate a response.\n\nAssuming you have already loaded data into a `VectorStore`, you can perform Retrieval Augmented Generation (RAG) by providing an instance of `QuestionAnswerAdvisor` to the `ChatClient`.\n\n```java hljs\nChatResponse response = ChatClient.builder(chatModel)\n        .build().prompt()\n        .advisors(new QuestionAnswerAdvisor(vectorStore))\n        .user(userText)\n        .call()\n        .chatResponse();\nCopied!\n```\n\nIn this example, the `QuestionAnswerAdvisor` will perform a similarity search over all documents in the Vector Database. To restrict the types of documents that are searched, the `SearchRequest` takes an SQL like filter expression that is portable across all `VectorStores`.\n\nThis filter expression can be configured when creating the `QuestionAnswerAdvisor` and hence will always apply to all `ChatClient` requests, or it can be provided at runtime per request.\n\nHere is how to create an instance of `QuestionAnswerAdvisor` where the threshold is `0.8` and to return the top `6` results.\n\n```java hljs\nvar qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore)\n        .searchRequest(SearchRequest.builder().similarityThreshold(0.8d).topK(6).build())\n        .build();\nCopied!\n```\n\n#### Dynamic Filter Expressions\n\nUpdate the `SearchRequest` filter expression at runtime using the `FILTER_EXPRESSION` advisor context parameter:\n\n```java hljs\nChatClient chatClient = ChatClient.builder(chatModel)\n    .defaultAdvisors(QuestionAnswerAdvisor.builder(vectorStore)\n        .searchRequest(SearchRequest.builder().build())\n        .build())\n    .build();\n\n// Update filter expression at runtime\nString content = this.chatClient.prompt()\n    .user(\"Please answer my question XYZ\")\n    .advisors(a -> a.param(QuestionAnswerAdvisor.FILTER_EXPRESSION, \"type == 'Spring'\"))\n    .call()\n    .content();\nCopied!\n```\n\nThe `FILTER_EXPRESSION` parameter allows you to dynamically filter the search results based on the provided expression.\n\n#### Custom Template\n\nThe `QuestionAnswerAdvisor` uses a default template to augment the user question with the retrieved documents. You can customize this behavior by providing your own `PromptTemplate` object via the `.promptTemplate()` builder method.\n\n|     |     |\n| --- | --- |\n|  | The `PromptTemplate` provided here customizes how the advisor merges retrieved context with the user query. This is distinct from configuring a `TemplateRenderer` on the `ChatClient` itself (using `.templateRenderer()`), which affects the rendering of the initial user/system prompt content **before** the advisor runs. See [ChatClient Prompt Templates](https://docs.spring.io/spring-ai/reference/api/chatclient.html#_prompt_templates) for more details on client-level template rendering. |\n\nThe custom `PromptTemplate` can use any `TemplateRenderer` implementation (by default, it uses `StPromptTemplate` based on the [StringTemplate](https://www.stringtemplate.org/) engine). The important requirement is that the template must contain the following two placeholders:\n\n- a `query` placeholder to receive the user question.\n\n- a `question_answer_context` placeholder to receive the retrieved context.\n\n\n```java hljs\nPromptTemplate customPromptTemplate = PromptTemplate.builder()\n    .renderer(StTemplateRenderer.builder().startDelimiterToken('<').endDelimiterToken('>').build())\n    .template(\"\"\"\n            <query>\n\n            Context information is below.\n\n\t\t\t---------------------\n\t\t\t<question_answer_context>\n\t\t\t---------------------\n\n\t\t\tGiven the context information and no prior knowledge, answer the query.\n\n\t\t\tFollow these rules:\n\n\t\t\t1. If the answer is not in the context, just say that you don't know.\n\t\t\t2. Avoid statements like \"Based on the context...\" or \"The provided information...\".\n            \"\"\")\n    .build();\n\n    String question = \"Where does the adventure of Anacletus and Birba take place?\";\n\n    QuestionAnswerAdvisor qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore)\n        .promptTemplate(customPromptTemplate)\n        .build();\n\n    String response = ChatClient.builder(chatModel).build()\n        .prompt(question)\n        .advisors(qaAdvisor)\n        .call()\n        .content();\nCopied!\n```\n\n|     |     |\n| --- | --- |\n|  | The `QuestionAnswerAdvisor.Builder.userTextAdvise()` method is deprecated in favor of using `.promptTemplate()` for more flexible customization. |\n\n### RetrievalAugmentationAdvisor\n\nSpring AI includes a [library of RAG modules](https://docs.spring.io/spring-ai/reference/api/retrieval-augmented-generation.html#modules) that you can use to build your own RAG flows.\nThe `RetrievalAugmentationAdvisor` is an `Advisor` providing an out-of-the-box implementation for the most common RAG flows,\nbased on a modular architecture.\n\n#### Sequential RAG Flows\n\n##### Naive RAG\n\n```java hljs\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder()\n        .documentRetriever(VectorStoreDocumentRetriever.builder()\n                .similarityThreshold(0.50)\n                .vectorStore(vectorStore)\n                .build())\n        .build();\n\nString answer = chatClient.prompt()\n        .advisors(retrievalAugmentationAdvisor)\n        .user(question)\n        .call()\n        .content();\nCopied!\n```\n\nBy default, the `RetrievalAugmentationAdvisor` does not allow the retrieved context to be empty. When that happens,\nit instructs the model not to answer the user query. You can allow empty context as follows.\n\n```java hljs\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder()\n        .documentRetriever(VectorStoreDocumentRetriever.builder()\n                .similarityThreshold(0.50)\n                .vectorStore(vectorStore)\n                .build())\n        .queryAugmenter(ContextualQueryAugmenter.builder()\n                .allowEmptyContext(true)\n                .build())\n        .build();\n\nString answer = chatClient.prompt()\n        .advisors(retrievalAugmentationAdvisor)\n        .user(question)\n        .call()\n        .content();\nCopied!\n```\n\nThe `VectorStoreDocumentRetriever` accepts a `FilterExpression` to filter the search results based on metadata.\nYou can provide one when instantiating the `VectorStoreDocumentRetriever` or at runtime per request,\nusing the `FILTER_EXPRESSION` advisor context parameter.\n\n```java hljs\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder()\n        .documentRetriever(VectorStoreDocumentRetriever.builder()\n                .similarityThreshold(0.50)\n                .vectorStore(vectorStore)\n                .build())\n        .build();\n\nString answer = chatClient.prompt()\n        .advisors(retrievalAugmentationAdvisor)\n        .advisors(a -> a.param(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \"type == 'Spring'\"))\n        .user(question)\n        .call()\n        .content();\nCopied!\n```\n\nSee [VectorStoreDocumentRetriever](https://docs.spring.io/spring-ai/reference/api/retrieval-augmented-generation.html#_vectorstoredocumentretriever) for more information.\n\n##### Advanced RAG\n\n```java hljs\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder()\n        .queryTransformers(RewriteQueryTransformer.builder()\n                .chatClientBuilder(chatClientBuilder.build().mutate())\n                .build())\n        .documentRetriever(VectorStoreDocumentRetriever.builder()\n                .similarityThreshold(0.50)\n                .vectorStore(vectorStore)\n                .build())\n        .build();\n\nString answer = chatClient.prompt()\n        .advisors(retrievalAugmentationAdvisor)\n        .user(question)\n        .call()\n        .content();\nCopied!\n```\n\nYou can also use the `DocumentPostProcessor` API to post-process the retrieved documents before passing them to the model. For example, you can use such an interface to perform re-ranking of the retrieved documents based on their relevance to the query, remove irrelevant or redundant documents, or compress the content of each document to reduce noise and redundancy.\n\n## Modules\n\nSpring AI implements a Modular RAG architecture inspired by the concept of modularity detailed in the paper\n\" [Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/abs/2407.21059)\".\n\n### Pre-Retrieval\n\nPre-Retrieval modules are responsible for processing the user query to achieve the best possible retrieval results.\n\n#### Query Transformation\n\nA component for transforming the input query to make it more effective for retrieval tasks, addressing challenges\nsuch as poorly formed queries, ambiguous terms, complex vocabulary, or unsupported languages.\n\n|     |     |\n| --- | --- |\n|  | When using a `QueryTransformer`, it\u2019s recommended to configure the `ChatClient.Builder` with a low temperature (e.g., 0.0) to ensure more deterministic and accurate results, improving retrieval quality. The default temperature for most chat models is typically too high for optimal query transformation, leading to reduced retrieval effectiveness. |\n\n##### CompressionQueryTransformer\n\nA `CompressionQueryTransformer` uses a large language model to compress a conversation history and a follow-up query\ninto a standalone query that captures the essence of the conversation.\n\nThis transformer is useful when the conversation history is long and the follow-up query is related\nto the conversation context.\n\n```java hljs\nQuery query = Query.builder()\n        .text(\"And what is its second largest city?\")\n        .history(new UserMessage(\"What is the capital of Denmark?\"),\n                new AssistantMessage(\"Copenhagen is the capital of Denmark.\"))\n        .build();\n\nQueryTransformer queryTransformer = CompressionQueryTransformer.builder()\n        .chatClientBuilder(chatClientBuilder)\n        .build();\n\nQuery transformedQuery = queryTransformer.transform(query);\nCopied!\n```\n\nThe prompt used by this component can be customized via the `promptTemplate()` method available in the builder.\n\n##### RewriteQueryTransformer\n\nA `RewriteQueryTransformer` uses a large language model to rewrite a user query to provide better results when\nquerying a target system, such as a vector store or a web search engine.\n\nThis transformer is useful when the user query is verbose, ambiguous, or contains irrelevant information\nthat may affect the quality of the search results.\n\n```java hljs\nQuery query = new Query(\"I'm studying machine learning. What is an LLM?\");\n\nQueryTransformer queryTransformer = RewriteQueryTransformer.builder()\n        .chatClientBuilder(chatClientBuilder)\n        .build();\n\nQuery transformedQuery = queryTransformer.transform(query);\nCopied!\n```\n\nThe prompt used by this component can be customized via the `promptTemplate()` method available in the builder.\n\n##### TranslationQueryTransformer\n\nA `TranslationQueryTransformer` uses a large language model to translate a query to a target language that is supported\nby the embedding model used to generate the document embeddings. If the query is already in the target language,\nit is returned unchanged. If the language of the query is unknown, it is also returned unchanged.\n\nThis transformer is useful when the embedding model is trained on a specific language and the user query\nis in a different language.\n\n```java hljs\nQuery query = new Query(\"Hvad er Danmarks hovedstad?\");\n\nQueryTransformer queryTransformer = TranslationQueryTransformer.builder()\n        .chatClientBuilder(chatClientBuilder)\n        .targetLanguage(\"english\")\n        .build();\n\nQuery transformedQuery = queryTransformer.transform(query);\nCopied!\n```\n\nThe prompt used by this component can be customized via the `promptTemplate()` method available in the builder.\n\n#### Query Expansion\n\nA component for expanding the input query into a list of queries, addressing challenges such as poorly formed queries\nby providing alternative query formulations, or by breaking down complex problems into simpler sub-queries.\n\n##### MultiQueryExpander\n\nA `MultiQueryExpander` uses a large language model to expand a query into multiple semantically diverse variations\nto capture different perspectives, useful for retrieving additional contextual information and increasing the chances\nof finding relevant results.\n\n```java hljs\nMultiQueryExpander queryExpander = MultiQueryExpander.builder()\n    .chatClientBuilder(chatClientBuilder)\n    .numberOfQueries(3)\n    .build();\nList<Query> queries = queryExpander.expand(new Query(\"How to run a Spring Boot app?\"));\nCopied!\n```\n\nBy default, the `MultiQueryExpander` includes the original query in the list of expanded queries. You can disable this behavior\nvia the `includeOriginal` method in the builder.\n\n```java hljs\nMultiQueryExpander queryExpander = MultiQueryExpander.builder()\n    .chatClientBuilder(chatClientBuilder)\n    .includeOriginal(false)\n    .build();\nCopied!\n```\n\nThe prompt used by this component can be customized via the `promptTemplate()` method available in the builder.\n\n### Retrieval\n\nRetrieval modules are responsible for querying data systems like vector store and retrieving the most relevant documents.\n\n#### Document Search\n\nComponent responsible for retrieving `Documents` from an underlying data source, such as a search engine, a vector store,\na database, or a knowledge graph.\n\n##### VectorStoreDocumentRetriever\n\nA `VectorStoreDocumentRetriever` retrieves documents from a vector store that are semantically similar to the input\nquery. It supports filtering based on metadata, similarity threshold, and top-k results.\n\n```java hljs\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder()\n    .vectorStore(vectorStore)\n    .similarityThreshold(0.73)\n    .topK(5)\n    .filterExpression(new FilterExpressionBuilder()\n        .eq(\"genre\", \"fairytale\")\n        .build())\n    .build();\nList<Document> documents = retriever.retrieve(new Query(\"What is the main character of the story?\"));\nCopied!\n```\n\nThe filter expression can be static or dynamic. For dynamic filter expressions, you can pass a `Supplier`.\n\n```java hljs\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder()\n    .vectorStore(vectorStore)\n    .filterExpression(() -> new FilterExpressionBuilder()\n        .eq(\"tenant\", TenantContextHolder.getTenantIdentifier())\n        .build())\n    .build();\nList<Document> documents = retriever.retrieve(new Query(\"What are the KPIs for the next semester?\"));\nCopied!\n```\n\nYou can also provide a request-specific filter expression via the `Query` API, using the `FILTER_EXPRESSION` parameter.\nIf both the request-specific and the retriever-specific filter expressions are provided, the request-specific filter expression takes precedence.\n\n```java hljs\nQuery query = Query.builder()\n    .text(\"Who is Anacletus?\")\n    .context(Map.of(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \"location == 'Whispering Woods'\"))\n    .build();\nList<Document> retrievedDocuments = documentRetriever.retrieve(query);\nCopied!\n```\n\n#### Document Join\n\nA component for combining documents retrieved based on multiple queries and from multiple data sources into\na single collection of documents. As part of the joining process, it can also handle duplicate documents and reciprocal\nranking strategies.\n\n##### ConcatenationDocumentJoiner\n\nA `ConcatenationDocumentJoiner` combines documents retrieved based on multiple queries and from multiple data sources\nby concatenating them into a single collection of documents. In case of duplicate documents, the first occurrence is kept.\nThe score of each document is kept as is.\n\n```java hljs\nMap<Query, List<List<Document>>> documentsForQuery = ...\nDocumentJoiner documentJoiner = new ConcatenationDocumentJoiner();\nList<Document> documents = documentJoiner.join(documentsForQuery);\nCopied!\n```\n\n### Post-Retrieval\n\nPost-Retrieval modules are responsible for processing the retrieved documents to achieve the best possible generation results.\n\n#### Document Post-Processing\n\nA component for post-processing retrieved documents based on a query, addressing challenges such as _lost-in-the-middle_, context length restrictions from the model, and the need to reduce noise and redundancy in the retrieved information.\n\nFor example, it could rank documents based on their relevance to the query, remove irrelevant or redundant documents, or compress the content of each document to reduce noise and redundancy.\n\n### Generation\n\nGeneration modules are responsible for generating the final response based on the user query and retrieved documents.\n\n#### Query Augmentation\n\nA component for augmenting an input query with additional data, useful to provide a large language model\nwith the necessary context to answer the user query.\n\n##### ContextualQueryAugmenter\n\nThe `ContextualQueryAugmenter` augments the user query with contextual data from the content of the provided documents.\n\n```java hljs\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder().build();\nCopied!\n```\n\nBy default, the `ContextualQueryAugmenter` does not allow the retrieved context to be empty. When that happens,\nit instructs the model not to answer the user query.\n\nYou can enable the `allowEmptyContext` option to allow the model to generate a response even when the retrieved context is empty.\n\n```java hljs\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder()\n        .allowEmptyContext(true)\n        .build();\nCopied!\n```\n\nThe prompts used by this component can be customized via the `promptTemplate()` and `emptyContextPromptTemplate()` methods\navailable in the builder."
  }
]